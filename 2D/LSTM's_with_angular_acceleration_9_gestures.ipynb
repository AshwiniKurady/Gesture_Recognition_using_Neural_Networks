{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ugVhCguDtM3T",
        "lhxFKnZRysSK",
        "e5TnEBqii0gZ",
        "j5xBMRZVtN7Q",
        "Fr4p7oQ8l5HG"
      ],
      "authorship_tag": "ABX9TyNcumuaVc1uJ2c7bFTOJle6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM-only model "
      ],
      "metadata": {
        "id": "ugVhCguDtM3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "import shutil\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "import csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Masking\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from matplotlib import pyplot\n",
        "import sys\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "cOlDakdAtNkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjXaLLDjq01i",
        "outputId": "9f77bc5d-6da3-4241-9fcc-c470d68f4776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load a single file as a numpy array\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def load_file(filepath):\n",
        "  tmpList= []\n",
        "  tmpList1 = []\n",
        "  tmpList2 = []\n",
        "  with open(filepath,'r') as thecsv:\n",
        "    for line in thecsv:\n",
        "      line = re.sub(re.compile(r'\\s+'), '', line)\n",
        "      line = line.strip(\",\")\n",
        "      tmpList.append(line.split(',')[0:24])\n",
        "\n",
        "    for i in range(len(tmpList)):\n",
        "      tmpList1 = []\n",
        "      for element in tmpList[i]:\n",
        "        tmpList1.append((float(element)))\n",
        "      tmpList2.append(tmpList1)\n",
        "  dataframe2 = pd.DataFrame(tmpList2)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3.csv')\n",
        "  dataframe2 = dataframe2.replace(np.nan,0.0, regex=True)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3with0.csv')\n",
        "  dataframe = dataframe2.astype(float)\n",
        "  return dataframe.values\n",
        "\n",
        "def load_file_y(filepath):\n",
        "  dataframe_y = read_csv(filepath, header=None)\n",
        "  file = open(filepath, \"r\")\n",
        "  csv_reader = csv. reader(file)\n",
        "  lists_from_csv = []\n",
        "  for row in csv_reader:\n",
        "    lists_from_csv. append(int(row[0])-1)\n",
        "  # return dataframe_y.values.tolist()\n",
        "  return lists_from_csv"
      ],
      "metadata": {
        "id": "BG3-xTLBq3pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "  loaded = list()\n",
        "  for name in filenames:\n",
        "    data = load_file(prefix + name)\n",
        "    loaded.append(data) \n",
        "  # stack group so that features are the 3rd dimension\n",
        "  # print(\"+++++++++++++++++++++++++\\n\\nBEFORE DSTACK\\n\\n++++++++++++++++++\")\n",
        "  loaded = dstack(loaded)\n",
        "  # print(\"+++++++++++++++++++++++++\\n\\nAFTER DSTACK\\n\\n++++++++++++++++++\")\n",
        "  # print(\"(Total samples, Timesteps, features) - \", loaded.shape)\n",
        "  # sys.exit(0)\n",
        "  return loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "  filepath = prefix + \"/\"\n",
        "  filenames = list()\n",
        "  filenames += ['leftshoulder_velocity.txt','leftshoulder_acceleration.txt','leftelbow_velocity.txt','leftelbow_acceleration.txt','leftwrist_velocity.txt', 'leftwrist_acceleration.txt',]\n",
        "  filenames += ['rightshoulder_velocity.txt','rightshoulder_acceleration.txt','rightelbow_velocity.txt','rightelbow_acceleration.txt','rightwrist_velocity.txt', 'rightwrist_acceleration.txt',]\n",
        "  # filenames += ['leftshoulder_polarangle.txt','leftshoulder_velocity.txt','leftelbow_polarangle.txt','leftelbow_velocity.txt','leftwrist_polarangle.txt', 'leftwrist_velocity.txt']\n",
        "  # filenames += ['rightshoulder_polarangle.txt','rightshoulder_velocity.txt','rightelbow_polarangle.txt','rightelbow_velocity.txt','rightwrist_polarangle.txt', 'rightwrist_velocity.txt']\n",
        "  # load input data\n",
        "  X = load_group(filenames, filepath)\n",
        "  # load class output\n",
        "  y = load_file_y( './drive/My Drive/CS298/2D' + '/y_9gestures.txt')\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "8TLEg2Kzq6OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix='./drive/My Drive/CS298/2D/angular_acceleration_6joints_9gestures'):\n",
        "  # load all train\n",
        "  # trainX, trainy = load_dataset_group('train', prefix)\n",
        "  # load all test\n",
        "  X, y = load_dataset_group('doesntmatter', prefix)\n",
        "  y = np.array(y)\n",
        "  sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=42)\n",
        "  for train_index, test_index in sss.split(X, y):\n",
        "    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "    trainX, testX = X[train_index], X[test_index]\n",
        "    trainy, testy = y[train_index], y[test_index]\n",
        "  # trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "  # zero-offset class values\n",
        "  # trainy = trainy - 1\n",
        "  # testy = testy - 1\n",
        "  # one hot encode y\n",
        "  #print(\"Before_Categorization\",trainy.shape,testy.shape)\n",
        "  trainy = to_categorical(trainy)\n",
        "  testy = to_categorical(testy)\n",
        "  #print(\"After_Categorization\",trainy.shape,testy.shape)\n",
        "  return trainX, trainy, testX, testy"
      ],
      "metadata": {
        "id": "mZ8xh5eiq8wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "  #better result for 50,271\n",
        "  verbose, epochs, batch_size = 0, 50, 271\n",
        "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "  print(\"Timesteps - \",n_timesteps, \"Features - \",n_features, \"Output Dimension - \",n_outputs)\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(tf.keras.layers.Masking(mask_value=0.0,input_shape=(n_timesteps, n_features)))\n",
        "  model.add(tf.keras.layers.LSTM(543))#######need to add the code to directly pick sample -> now it is manually written as 117\n",
        "  model.add(Dense(1000, activation='relu'))\n",
        "  model.add(Dense(n_outputs, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # fit network\n",
        "  model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "  # Confusion matrix\n",
        "  predictions = model.predict(testX)\n",
        "  # matrix = confusion_matrix(testy.argmax(axis=1), predictions.argmax(axis=1))\n",
        "  # print(matrix)\n",
        "  print(classification_report(testy.argmax(axis=1), predictions.argmax(axis=1)))\n",
        "  # evaluate model\n",
        "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=2)\n",
        "  # _, accuracy = model.evaluate(testX, testy, verbose=0)\n",
        "  # accuracy = 0\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "qhmhfdeLrBQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        "  print(scores)\n",
        "  m, s = mean(scores), std(scores)\n",
        "  print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
      ],
      "metadata": {
        "id": "1QcGPgJhrDf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run an experiment\n",
        "def run_experiment(repeats=3):\n",
        "  # load data\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  # print(trainX.shape, trainy.shape, testX.shape,testy.shape)\n",
        "  scores = list()\n",
        "  for r in range(repeats):\n",
        "    print(\"\\n\")\n",
        "    print(\"Running LSTM with Angular velocity and Acceleration - 9 gestures\")\n",
        "    score = evaluate_model(trainX, trainy, testX, testy)\n",
        "    score = score * 100.0\n",
        "    print('>#%d: %.3f' % (r+1, score))\n",
        "    scores.append(score)\n",
        "  # summarize results\n",
        "  summarize_results(scores)"
      ],
      "metadata": {
        "id": "2V1Mqa4CrF9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgQG1dsCrH4N",
        "outputId": "66d489ad-49af-4c8f-da31-a7d01d5b9038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Running LSTM with Angular velocity and Acceleration - 9 gestures\n",
            "Timesteps -  24 Features -  12 Output Dimension -  9\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.23      0.29        30\n",
            "           1       0.48      0.43      0.46        30\n",
            "           2       0.13      0.06      0.09        31\n",
            "           3       0.30      0.41      0.35        32\n",
            "           4       0.43      0.84      0.57        31\n",
            "           5       0.12      0.17      0.14        30\n",
            "           6       0.58      0.25      0.35        28\n",
            "           7       0.23      0.37      0.28        30\n",
            "           8       0.25      0.07      0.11        30\n",
            "\n",
            "    accuracy                           0.32       272\n",
            "   macro avg       0.32      0.31      0.29       272\n",
            "weighted avg       0.32      0.32      0.29       272\n",
            "\n",
            "2/2 - 2s - loss: 1.8060 - accuracy: 0.3162 - 2s/epoch - 864ms/step\n",
            ">#1: 31.618\n",
            "\n",
            "\n",
            "Running LSTM with Angular velocity and Acceleration - 9 gestures\n",
            "Timesteps -  24 Features -  12 Output Dimension -  9\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.03      0.06        30\n",
            "           1       0.29      0.30      0.30        30\n",
            "           2       0.00      0.00      0.00        31\n",
            "           3       0.09      0.12      0.10        32\n",
            "           4       0.32      0.84      0.46        31\n",
            "           5       0.00      0.00      0.00        30\n",
            "           6       0.53      0.32      0.40        28\n",
            "           7       0.10      0.30      0.15        30\n",
            "           8       0.50      0.03      0.06        30\n",
            "\n",
            "    accuracy                           0.22       272\n",
            "   macro avg       0.26      0.22      0.17       272\n",
            "weighted avg       0.25      0.22      0.17       272\n",
            "\n",
            "2/2 - 2s - loss: 2.3586 - accuracy: 0.2169 - 2s/epoch - 883ms/step\n",
            ">#2: 21.691\n",
            "\n",
            "\n",
            "Running LSTM with Angular velocity and Acceleration - 9 gestures\n",
            "Timesteps -  24 Features -  12 Output Dimension -  9\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.03      0.06        30\n",
            "           1       0.59      0.33      0.43        30\n",
            "           2       0.11      0.06      0.08        31\n",
            "           3       0.53      0.59      0.56        32\n",
            "           4       0.27      0.90      0.41        31\n",
            "           5       0.10      0.03      0.05        30\n",
            "           6       0.63      0.61      0.62        28\n",
            "           7       0.75      0.20      0.32        30\n",
            "           8       0.26      0.40      0.31        30\n",
            "\n",
            "    accuracy                           0.35       272\n",
            "   macro avg       0.40      0.35      0.31       272\n",
            "weighted avg       0.39      0.35      0.31       272\n",
            "\n",
            "2/2 - 2s - loss: 1.7701 - accuracy: 0.3529 - 2s/epoch - 897ms/step\n",
            ">#3: 35.294\n",
            "[31.617647409439087, 21.691176295280457, 35.29411852359772]\n",
            "Accuracy: 29.534% (+/-5.745)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN LSTM"
      ],
      "metadata": {
        "id": "lhxFKnZRysSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from matplotlib import pyplot\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "O0H122TMyt7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a single file as a numpy array\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def load_file(filepath):\n",
        "  tmpList= []\n",
        "  tmpList1 = []\n",
        "  tmpList2 = []\n",
        "  with open(filepath,'r') as thecsv:\n",
        "    for line in thecsv:\n",
        "      line = re.sub(re.compile(r'\\s+'), '', line)\n",
        "      line = line.strip(\",\")\n",
        "      tmpList.append(line.split(',')[0:24])\n",
        "\n",
        "    for i in range(len(tmpList)):\n",
        "      tmpList1 = []\n",
        "      for element in tmpList[i]:\n",
        "        tmpList1.append((float(element)))\n",
        "      tmpList2.append(tmpList1)\n",
        "  dataframe2 = pd.DataFrame(tmpList2)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3.csv')\n",
        "  dataframe2 = dataframe2.replace(np.nan,0.0, regex=True)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3with0.csv')\n",
        "  dataframe = dataframe2.astype(float)\n",
        "  return dataframe.values\n",
        "\n",
        "def load_file_y(filepath):\n",
        "  dataframe_y = read_csv(filepath, header=None)\n",
        "  file = open(filepath, \"r\")\n",
        "  csv_reader = csv. reader(file)\n",
        "  lists_from_csv = []\n",
        "  for row in csv_reader:\n",
        "    lists_from_csv. append(int(row[0])-1)\n",
        "  # return dataframe_y.values.tolist()\n",
        "  return lists_from_csv"
      ],
      "metadata": {
        "id": "122DpLoyyySb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "  loaded = list()\n",
        "  for name in filenames:\n",
        "    data = load_file(prefix + name)\n",
        "    loaded.append(data)\n",
        "\n",
        "  # stack group so that features are the 3rd dimension\n",
        "  loaded = dstack(loaded)\n",
        "  print(\"(Total samples, Timesteps, features) - \", loaded.shape)\n",
        "\n",
        "  return loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "  filepath = prefix + \"/\"\n",
        "  filenames = list()\n",
        "  filenames += ['leftshoulder_velocity.txt','leftshoulder_acceleration.txt','leftelbow_velocity.txt','leftelbow_acceleration.txt','leftwrist_velocity.txt', 'leftwrist_acceleration.txt',]\n",
        "  filenames += ['rightshoulder_velocity.txt','rightshoulder_acceleration.txt','rightelbow_velocity.txt','rightelbow_acceleration.txt','rightwrist_velocity.txt', 'rightwrist_acceleration.txt',]\n",
        "  # filenames += ['leftshoulder_polarangle.txt','leftshoulder_velocity.txt','leftelbow_polarangle.txt','leftelbow_velocity.txt','leftwrist_polarangle.txt', 'leftwrist_velocity.txt']\n",
        "  # filenames += ['rightshoulder_polarangle.txt','rightshoulder_velocity.txt','rightelbow_polarangle.txt','rightelbow_velocity.txt','rightwrist_polarangle.txt', 'rightwrist_velocity.txt']\n",
        "  # load input data\n",
        "  X = load_group(filenames, filepath)\n",
        "  # load class output\n",
        "  y = load_file_y( './drive/My Drive/CS298/2D' + '/y_9gestures.txt')\n",
        "  # print(\"Printing X -----------------------\")\n",
        "  # print(X)\n",
        "  # print(\"Printing y -----------------------\")\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "DCWbteQY17G0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix='./drive/My Drive/CS298/2D/angular_acceleration_6joints_9gestures'):\n",
        "  # load all train\n",
        "  # trainX, trainy = load_dataset_group('train', prefix)\n",
        "  # load all test\n",
        "  X, y = load_dataset_group('doesntmatter', prefix)\n",
        "  trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "  # zero-offset class values\n",
        "  #trainy = trainy - 1\n",
        "  #testy = testy - 1\n",
        "  # one hot encode y\n",
        "  trainy = to_categorical(trainy)\n",
        "  testy = to_categorical(testy)\n",
        "\n",
        "\n",
        "  return trainX, trainy, testX, testy"
      ],
      "metadata": {
        "id": "Ciax8OEz1-YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "  verbose, epochs, batch_size = 0, 100, 60\n",
        "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "  print(\"Timesteps - \",n_timesteps, \"Features - \",n_features, \"Output Dimension - \",n_outputs)\n",
        "  n_steps, n_length = 2,12\n",
        "  trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
        "  testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
        "\t# define model\n",
        "  model = Sequential()\n",
        "  model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n",
        "  # model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
        "  model.add(TimeDistributed(Dropout(0.5)))\n",
        "  model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        "  model.add(TimeDistributed(Flatten()))\n",
        "  model.add(LSTM(500))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(n_outputs, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# fit network\n",
        "  model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "\t# evaluate model\n",
        "  predictions = model.predict(testX)\n",
        "  # matrix = confusion_matrix(testy.argmax(axis=1), predictions.argmax(axis=1))\n",
        "  # print(matrix)\n",
        "  print(classification_report(testy.argmax(axis=1), predictions.argmax(axis=1)))\n",
        "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "  return accuracy\n",
        " "
      ],
      "metadata": {
        "id": "7ia3aKZ32JCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        "\tprint(scores)\n",
        "\tm, s = mean(scores), std(scores)\n",
        "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
      ],
      "metadata": {
        "id": "WuGJVNw82MTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run an experiment\n",
        "def run_experiment(repeats=3):\n",
        "  # load data\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  scores = list()\n",
        "  for r in range(repeats):\n",
        "    print(\"\\n\")\n",
        "    print(\"Running CNN-LSTM with Angular velocity and Acceleration-2D - 9 gestures\")\n",
        "    score = evaluate_model(trainX, trainy, testX, testy)\n",
        "    score = score * 100.0\n",
        "    print('>#%d: %.3f' % (r+1, score))\n",
        "    scores.append(score)\n",
        "  # summarize results\n",
        "  summarize_results(scores)"
      ],
      "metadata": {
        "id": "HdgRwsrz2Po6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feDUOp5T2TOh",
        "outputId": "8925cd4b-8c1d-4968-a683-e9d7b416cf0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Total samples, Timesteps, features) -  (543, 24, 12)\n",
            "\n",
            "\n",
            "Running CNN-LSTM with Angular velocity and Acceleration-2D - 9 gestures\n",
            "Timesteps -  24 Features -  12 Output Dimension -  9\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.29      0.42        14\n",
            "           1       0.75      0.75      0.75        20\n",
            "           2       0.40      0.46      0.43        13\n",
            "           3       0.60      0.80      0.69        15\n",
            "           4       0.92      0.92      0.92        13\n",
            "           5       0.36      0.31      0.33        13\n",
            "           6       0.92      0.80      0.86        15\n",
            "           7       0.48      0.93      0.63        14\n",
            "           8       0.67      0.42      0.52        19\n",
            "\n",
            "    accuracy                           0.63       136\n",
            "   macro avg       0.66      0.63      0.62       136\n",
            "weighted avg       0.66      0.63      0.62       136\n",
            "\n",
            ">#1: 63.235\n",
            "\n",
            "\n",
            "Running CNN-LSTM with Angular velocity and Acceleration-2D - 9 gestures\n",
            "Timesteps -  24 Features -  12 Output Dimension -  9\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.43      0.57        14\n",
            "           1       0.76      0.80      0.78        20\n",
            "           2       0.39      0.54      0.45        13\n",
            "           3       0.86      0.80      0.83        15\n",
            "           4       0.85      0.85      0.85        13\n",
            "           5       0.45      0.69      0.55        13\n",
            "           6       0.87      0.87      0.87        15\n",
            "           7       0.68      0.93      0.79        14\n",
            "           8       0.89      0.42      0.57        19\n",
            "\n",
            "    accuracy                           0.70       136\n",
            "   macro avg       0.73      0.70      0.69       136\n",
            "weighted avg       0.75      0.70      0.70       136\n",
            "\n",
            ">#2: 69.853\n",
            "\n",
            "\n",
            "Running CNN-LSTM with Angular velocity and Acceleration-2D - 9 gestures\n",
            "Timesteps -  24 Features -  12 Output Dimension -  9\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.43      0.57        14\n",
            "           1       0.82      0.70      0.76        20\n",
            "           2       0.40      0.31      0.35        13\n",
            "           3       0.80      0.80      0.80        15\n",
            "           4       0.65      0.85      0.73        13\n",
            "           5       0.50      0.77      0.61        13\n",
            "           6       0.73      0.73      0.73        15\n",
            "           7       0.69      0.79      0.73        14\n",
            "           8       0.68      0.68      0.68        19\n",
            "\n",
            "    accuracy                           0.68       136\n",
            "   macro avg       0.68      0.67      0.66       136\n",
            "weighted avg       0.69      0.68      0.67       136\n",
            "\n",
            ">#3: 67.647\n",
            "[63.235294818878174, 69.85294222831726, 67.64705777168274]\n",
            "Accuracy: 66.912% (+/-2.751)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConvLSTM"
      ],
      "metadata": {
        "id": "e5TnEBqii0gZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import ConvLSTM2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from matplotlib import pyplot"
      ],
      "metadata": {
        "id": "pl1a2Doaifdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a single file as a numpy array\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def load_file(filepath):\n",
        "  tmpList= []\n",
        "  tmpList1 = []\n",
        "  tmpList2 = []\n",
        "  with open(filepath,'r') as thecsv:\n",
        "    for line in thecsv:\n",
        "      line = re.sub(re.compile(r'\\s+'), '', line)\n",
        "      line = line.strip(\",\")\n",
        "      tmpList.append(line.split(',')[0:24])\n",
        "\n",
        "    for i in range(len(tmpList)):\n",
        "      tmpList1 = []\n",
        "      for element in tmpList[i]:\n",
        "        tmpList1.append((float(element)))\n",
        "      tmpList2.append(tmpList1)\n",
        "  dataframe2 = pd.DataFrame(tmpList2)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3.csv')\n",
        "  dataframe2 = dataframe2.replace(np.nan,0.0, regex=True)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3with0.csv')\n",
        "  dataframe = dataframe2.astype(float)\n",
        "  return dataframe.values\n",
        "\n",
        "def load_file_y(filepath):\n",
        "  dataframe_y = read_csv(filepath, header=None)\n",
        "  file = open(filepath, \"r\")\n",
        "  csv_reader = csv. reader(file)\n",
        "  lists_from_csv = []\n",
        "  for row in csv_reader:\n",
        "    lists_from_csv. append(int(row[0])-1)\n",
        "  # return dataframe_y.values.tolist()\n",
        "  return lists_from_csv"
      ],
      "metadata": {
        "id": "NiGj6V3cigit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "  loaded = list()\n",
        "  for name in filenames:\n",
        "    data = load_file(prefix + name)\n",
        "    loaded.append(data)\n",
        "\n",
        "  # stack group so that features are the 3rd dimension\n",
        "  loaded = dstack(loaded)\n",
        "  print(\"(Total samples, Timesteps, features) - \", loaded.shape)\n",
        "\n",
        "  return loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "  filepath = prefix + \"/\"\n",
        "  filenames = list()\n",
        "  filenames += ['leftshoulder_velocity.txt','leftshoulder_acceleration.txt','leftelbow_velocity.txt','leftelbow_acceleration.txt','leftwrist_velocity.txt', 'leftwrist_acceleration.txt',]\n",
        "  filenames += ['rightshoulder_velocity.txt','rightshoulder_acceleration.txt','rightelbow_velocity.txt','rightelbow_acceleration.txt','rightwrist_velocity.txt', 'rightwrist_acceleration.txt',]\n",
        "  # filenames += ['leftshoulder_polarangle.txt','leftshoulder_velocity.txt','leftelbow_polarangle.txt','leftelbow_velocity.txt','leftwrist_polarangle.txt', 'leftwrist_velocity.txt']\n",
        "  # filenames += ['rightshoulder_polarangle.txt','rightshoulder_velocity.txt','rightelbow_polarangle.txt','rightelbow_velocity.txt','rightwrist_polarangle.txt', 'rightwrist_velocity.txt']\n",
        "  # load input data\n",
        "  X = load_group(filenames, filepath)\n",
        "  # load class output\n",
        "  y = load_file_y( './drive/My Drive/CS298/2D' + '/y_9gestures.txt')\n",
        "  # print(\"Printing X -----------------------\")\n",
        "  # print(X)\n",
        "  # print(\"Printing y -----------------------\")\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "28RcfOMFij1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix='./drive/My Drive/CS298/2D/angular_acceleration_6joints_9gestures'):\n",
        "  # load all train\n",
        "  # trainX, trainy = load_dataset_group('train', prefix)\n",
        "  # load all test\n",
        "  X, y = load_dataset_group('doesntmatter', prefix)\n",
        "  trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "  # zero-offset class values\n",
        "  #trainy = trainy - 1\n",
        "  #testy = testy - 1\n",
        "  # one hot encode y\n",
        "  trainy = to_categorical(trainy)\n",
        "  testy = to_categorical(testy)\n",
        "\n",
        "\n",
        "  return trainX, trainy, testX, testy"
      ],
      "metadata": {
        "id": "nILG5Jmqimfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import LeakyReLU\n",
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "  # define model\n",
        "  verbose, epochs, batch_size = 0, 100, 60\n",
        "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "  # reshape into subsequences (samples, time steps, rows, cols, channels)\n",
        "  n_steps, n_length = 2,12\n",
        "  trainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n",
        "  testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  model.add(ConvLSTM2D(filters=30, kernel_size=(1,1), input_shape=(n_steps, 1, n_length, n_features)))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(100,activation='relu'))\n",
        "  model.add(Dense(n_outputs, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # fit network\n",
        "  model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "  # evaluate model\n",
        "  predictions = model.predict(testX)\n",
        "  print(classification_report(testy.argmax(axis=1),predictions.argmax(axis=1)))\n",
        "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "JIebyQ7LipZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        "\tprint(scores)\n",
        "\tm, s = mean(scores), std(scores)\n",
        "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
      ],
      "metadata": {
        "id": "mv6r8m-risOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run an experiment\n",
        "def run_experiment(repeats=3):\n",
        "  # load data\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  scores = list()\n",
        "  for r in range(repeats):\n",
        "    print(\"\\n\")\n",
        "    print(\"Running ConvLSTM with Angular velocity and Acceleration-2D - 9 gestures\")\n",
        "    score = evaluate_model(trainX, trainy, testX, testy)\n",
        "    score = score * 100.0\n",
        "    print('>#%d: %.3f' % (r+1, score))\n",
        "    scores.append(score)\n",
        "  # summarize results\n",
        "  summarize_results(scores)"
      ],
      "metadata": {
        "id": "O1LX2FNFivGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSxfpCdZiyGA",
        "outputId": "bdf8ad8c-27c3-43dc-9e1b-e35b8ff2da34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Total samples, Timesteps, features) -  (543, 24, 12)\n",
            "\n",
            "\n",
            "Running ConvLSTM with Angular velocity and Acceleration-2D - 9 gestures\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.43      0.43        14\n",
            "           1       0.72      0.65      0.68        20\n",
            "           2       0.56      0.69      0.62        13\n",
            "           3       0.75      0.60      0.67        15\n",
            "           4       0.87      1.00      0.93        13\n",
            "           5       0.38      0.46      0.41        13\n",
            "           6       0.92      0.80      0.86        15\n",
            "           7       0.52      0.79      0.63        14\n",
            "           8       0.91      0.53      0.67        19\n",
            "\n",
            "    accuracy                           0.65       136\n",
            "   macro avg       0.67      0.66      0.65       136\n",
            "weighted avg       0.69      0.65      0.66       136\n",
            "\n",
            ">#1: 65.441\n",
            "\n",
            "\n",
            "Running ConvLSTM with Angular velocity and Acceleration-2D - 9 gestures\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.57      0.57        14\n",
            "           1       0.80      0.60      0.69        20\n",
            "           2       0.50      0.62      0.55        13\n",
            "           3       0.64      0.60      0.62        15\n",
            "           4       0.85      0.85      0.85        13\n",
            "           5       0.45      0.69      0.55        13\n",
            "           6       0.92      0.80      0.86        15\n",
            "           7       0.63      0.86      0.73        14\n",
            "           8       0.92      0.58      0.71        19\n",
            "\n",
            "    accuracy                           0.68       136\n",
            "   macro avg       0.70      0.68      0.68       136\n",
            "weighted avg       0.71      0.68      0.68       136\n",
            "\n",
            ">#2: 67.647\n",
            "\n",
            "\n",
            "Running ConvLSTM with Angular velocity and Acceleration-2D - 9 gestures\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.57      0.55        14\n",
            "           1       0.87      0.65      0.74        20\n",
            "           2       0.46      0.46      0.46        13\n",
            "           3       0.77      0.67      0.71        15\n",
            "           4       0.93      1.00      0.96        13\n",
            "           5       0.59      0.77      0.67        13\n",
            "           6       0.71      0.67      0.69        15\n",
            "           7       0.64      1.00      0.78        14\n",
            "           8       0.85      0.58      0.69        19\n",
            "\n",
            "    accuracy                           0.70       136\n",
            "   macro avg       0.70      0.71      0.69       136\n",
            "weighted avg       0.72      0.70      0.70       136\n",
            "\n",
            ">#3: 69.853\n",
            "[65.4411792755127, 67.64705777168274, 69.85294222831726]\n",
            "Accuracy: 67.647% (+/-1.801)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConvLSTM - LeakyReLU"
      ],
      "metadata": {
        "id": "j5xBMRZVtN7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import ConvLSTM2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from matplotlib import pyplot"
      ],
      "metadata": {
        "id": "FSmC_PdmtP5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a single file as a numpy array\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def load_file(filepath):\n",
        "  tmpList= []\n",
        "  tmpList1 = []\n",
        "  tmpList2 = []\n",
        "  with open(filepath,'r') as thecsv:\n",
        "    for line in thecsv:\n",
        "      line = re.sub(re.compile(r'\\s+'), '', line)\n",
        "      line = line.strip(\",\")\n",
        "      tmpList.append(line.split(',')[0:24])\n",
        "\n",
        "    for i in range(len(tmpList)):\n",
        "      tmpList1 = []\n",
        "      for element in tmpList[i]:\n",
        "        tmpList1.append((float(element)))\n",
        "      tmpList2.append(tmpList1)\n",
        "  dataframe2 = pd.DataFrame(tmpList2)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3.csv')\n",
        "  dataframe2 = dataframe2.replace(np.nan,0.0, regex=True)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3with0.csv')\n",
        "  dataframe = dataframe2.astype(float)\n",
        "  return dataframe.values\n",
        "\n",
        "def load_file_y(filepath):\n",
        "  dataframe_y = read_csv(filepath, header=None)\n",
        "  file = open(filepath, \"r\")\n",
        "  csv_reader = csv. reader(file)\n",
        "  lists_from_csv = []\n",
        "  for row in csv_reader:\n",
        "    lists_from_csv. append(int(row[0])-1)\n",
        "  # return dataframe_y.values.tolist()\n",
        "  return lists_from_csv"
      ],
      "metadata": {
        "id": "l0nQ52HTtSC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "  loaded = list()\n",
        "  for name in filenames:\n",
        "    data = load_file(prefix + name)\n",
        "    loaded.append(data)\n",
        "\n",
        "  # stack group so that features are the 3rd dimension\n",
        "  loaded = dstack(loaded)\n",
        "  print(\"(Total samples, Timesteps, features) - \", loaded.shape)\n",
        "\n",
        "  return loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "  filepath = prefix + \"/\"\n",
        "  filenames = list()\n",
        "  filenames += ['leftshoulder_velocity.txt','leftshoulder_acceleration.txt','leftelbow_velocity.txt','leftelbow_acceleration.txt','leftwrist_velocity.txt', 'leftwrist_acceleration.txt',]\n",
        "  filenames += ['rightshoulder_velocity.txt','rightshoulder_acceleration.txt','rightelbow_velocity.txt','rightelbow_acceleration.txt','rightwrist_velocity.txt', 'rightwrist_acceleration.txt',]\n",
        "  # filenames += ['leftshoulder_polarangle.txt','leftshoulder_velocity.txt','leftelbow_polarangle.txt','leftelbow_velocity.txt','leftwrist_polarangle.txt', 'leftwrist_velocity.txt']\n",
        "  # filenames += ['rightshoulder_polarangle.txt','rightshoulder_velocity.txt','rightelbow_polarangle.txt','rightelbow_velocity.txt','rightwrist_polarangle.txt', 'rightwrist_velocity.txt']\n",
        "  # load input data\n",
        "  X = load_group(filenames, filepath)\n",
        "  # load class output\n",
        "  y = load_file_y( './drive/My Drive/CS298/2D' + '/y_9gestures.txt')\n",
        "  # print(\"Printing X -----------------------\")\n",
        "  # print(X)\n",
        "  # print(\"Printing y -----------------------\")\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "Uhk9rCmvtUCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix='./drive/My Drive/CS298/2D/angular_acceleration_6joints_9gestures'):\n",
        "  # load all train\n",
        "  # trainX, trainy = load_dataset_group('train', prefix)\n",
        "  # load all test\n",
        "  X, y = load_dataset_group('doesntmatter', prefix)\n",
        "  trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "  # zero-offset class values\n",
        "  #trainy = trainy - 1\n",
        "  #testy = testy - 1\n",
        "  # one hot encode y\n",
        "  trainy = to_categorical(trainy)\n",
        "  testy = to_categorical(testy)\n",
        "\n",
        "\n",
        "  return trainX, trainy, testX, testy"
      ],
      "metadata": {
        "id": "YRGetXvRtXLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import LeakyReLU\n",
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "  # define model\n",
        "  verbose, epochs, batch_size = 0, 100, 60\n",
        "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "  # reshape into subsequences (samples, time steps, rows, cols, channels)\n",
        "  n_steps, n_length = 2,12\n",
        "  trainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n",
        "  testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  model.add(ConvLSTM2D(filters=30, kernel_size=(1,1), input_shape=(n_steps, 1, n_length, n_features)))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(LeakyReLU(alpha=0.05))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(100))\n",
        "  model.add(Dense(n_outputs, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # fit network\n",
        "  model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "  # evaluate model\n",
        "  predictions = model.predict(testX)\n",
        "  print(classification_report(testy.argmax(axis=1),predictions.argmax(axis=1)))\n",
        "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "3K5bkRjy8uEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        "\tprint(scores)\n",
        "\tm, s = mean(scores), std(scores)\n",
        "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
      ],
      "metadata": {
        "id": "DHEJlFazta3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run an experiment\n",
        "def run_experiment(repeats=3):\n",
        "  # load data\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  scores = list()\n",
        "  for r in range(repeats):\n",
        "    print(\"\\n\")\n",
        "    print(\"Running ConvLSTM with Angular velocity and Acceleration-2D - 9 gestures\")\n",
        "    score = evaluate_model(trainX, trainy, testX, testy)\n",
        "    score = score * 100.0\n",
        "    print('>#%d: %.3f' % (r+1, score))\n",
        "    scores.append(score)\n",
        "  # summarize results\n",
        "  summarize_results(scores)"
      ],
      "metadata": {
        "id": "EfIsKvIxtclg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_experiment()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKdLuFxhteXp",
        "outputId": "0964c004-6724-43c6-f64b-8bd9117493e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Total samples, Timesteps, features) -  (543, 24, 12)\n",
            "\n",
            "\n",
            "Running ConvLSTM with Angular velocity and Acceleration-2D - 9 gestures\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.43      0.46        14\n",
            "           1       0.76      0.65      0.70        20\n",
            "           2       0.40      0.46      0.43        13\n",
            "           3       0.71      0.67      0.69        15\n",
            "           4       0.92      0.92      0.92        13\n",
            "           5       0.53      0.69      0.60        13\n",
            "           6       0.86      0.80      0.83        15\n",
            "           7       0.73      0.79      0.76        14\n",
            "           8       0.63      0.63      0.63        19\n",
            "\n",
            "    accuracy                           0.67       136\n",
            "   macro avg       0.67      0.67      0.67       136\n",
            "weighted avg       0.68      0.67      0.67       136\n",
            "\n",
            ">#1: 66.912\n",
            "\n",
            "\n",
            "Running ConvLSTM with Angular velocity and Acceleration-2D - 9 gestures\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.57      0.55        14\n",
            "           1       0.93      0.70      0.80        20\n",
            "           2       0.39      0.69      0.50        13\n",
            "           3       0.82      0.60      0.69        15\n",
            "           4       0.93      1.00      0.96        13\n",
            "           5       0.47      0.62      0.53        13\n",
            "           6       0.82      0.93      0.87        15\n",
            "           7       0.77      0.71      0.74        14\n",
            "           8       0.73      0.42      0.53        19\n",
            "\n",
            "    accuracy                           0.68       136\n",
            "   macro avg       0.71      0.69      0.69       136\n",
            "weighted avg       0.73      0.68      0.69       136\n",
            "\n",
            ">#2: 68.382\n",
            "\n",
            "\n",
            "Running ConvLSTM with Angular velocity and Acceleration-2D - 9 gestures\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.71      0.61        14\n",
            "           1       0.93      0.70      0.80        20\n",
            "           2       0.41      0.69      0.51        13\n",
            "           3       0.65      0.73      0.69        15\n",
            "           4       0.92      0.85      0.88        13\n",
            "           5       0.57      0.62      0.59        13\n",
            "           6       0.91      0.67      0.77        15\n",
            "           7       0.86      0.86      0.86        14\n",
            "           8       0.83      0.53      0.65        19\n",
            "\n",
            "    accuracy                           0.70       136\n",
            "   macro avg       0.73      0.71      0.71       136\n",
            "weighted avg       0.75      0.70      0.71       136\n",
            "\n",
            ">#3: 69.853\n",
            "[66.91176295280457, 68.38235259056091, 69.85294222831726]\n",
            "Accuracy: 68.382% (+/-1.201)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConvLSTM - LeakyReLU with increase in noise data"
      ],
      "metadata": {
        "id": "Fr4p7oQ8l5HG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import ConvLSTM2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from matplotlib import pyplot"
      ],
      "metadata": {
        "id": "LGkxFa3rmBNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a single file as a numpy array\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def load_file(filepath):\n",
        "  tmpList= []\n",
        "  tmpList1 = []\n",
        "  tmpList2 = []\n",
        "  with open(filepath,'r') as thecsv:\n",
        "    for line in thecsv:\n",
        "      line = re.sub(re.compile(r'\\s+'), '', line)\n",
        "      line = line.strip(\",\")\n",
        "      tmpList.append(line.split(',')[0:44])\n",
        "\n",
        "    for i in range(len(tmpList)):\n",
        "      tmpList1 = []\n",
        "      for element in tmpList[i]:\n",
        "        tmpList1.append((float(element)))\n",
        "      tmpList2.append(tmpList1)\n",
        "  dataframe2 = pd.DataFrame(tmpList2)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3.csv')\n",
        "  dataframe2 = dataframe2.replace(np.nan,0.0, regex=True)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3with0.csv')\n",
        "  dataframe = dataframe2.astype(float)\n",
        "  return dataframe.values\n",
        "\n",
        "def load_file_y(filepath):\n",
        "  dataframe_y = read_csv(filepath, header=None)\n",
        "  file = open(filepath, \"r\")\n",
        "  csv_reader = csv. reader(file)\n",
        "  lists_from_csv = []\n",
        "  for row in csv_reader:\n",
        "    lists_from_csv. append(int(row[0])-1)\n",
        "  # return dataframe_y.values.tolist()\n",
        "  return lists_from_csv"
      ],
      "metadata": {
        "id": "UYXy71csmM_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "  loaded = list()\n",
        "  for name in filenames:\n",
        "    data = load_file(prefix + name)\n",
        "    loaded.append(data)\n",
        "\n",
        "  # stack group so that features are the 3rd dimension\n",
        "  loaded = dstack(loaded)\n",
        "  print(\"(Total samples, Timesteps, features) - \", loaded.shape)\n",
        "\n",
        "  return loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "  filepath = prefix + \"/\"\n",
        "  filenames = list()\n",
        "  filenames += ['leftshoulder_velocity.txt','leftshoulder_acceleration.txt','leftelbow_velocity.txt','leftelbow_acceleration.txt','leftwrist_velocity.txt', 'leftwrist_acceleration.txt',]\n",
        "  filenames += ['rightshoulder_velocity.txt','rightshoulder_acceleration.txt','rightelbow_velocity.txt','rightelbow_acceleration.txt','rightwrist_velocity.txt', 'rightwrist_acceleration.txt',]\n",
        "  # filenames += ['leftshoulder_polarangle.txt','leftshoulder_velocity.txt','leftelbow_polarangle.txt','leftelbow_velocity.txt','leftwrist_polarangle.txt', 'leftwrist_velocity.txt']\n",
        "  # filenames += ['rightshoulder_polarangle.txt','rightshoulder_velocity.txt','rightelbow_polarangle.txt','rightelbow_velocity.txt','rightwrist_polarangle.txt', 'rightwrist_velocity.txt']\n",
        "  # load input data\n",
        "  X = load_group(filenames, filepath)\n",
        "  # load class output\n",
        "  y = load_file_y( './drive/My Drive/CS298/2D' + '/y_9gestures.txt')\n",
        "  # print(\"Printing X -----------------------\")\n",
        "  # print(X)\n",
        "  # print(\"Printing y -----------------------\")\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "NXLalB3Bn_aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix='./drive/My Drive/CS298/2D/angular_acceleration_6joints_9gestures'):\n",
        "  # load all train\n",
        "  # trainX, trainy = load_dataset_group('train', prefix)\n",
        "  # load all test\n",
        "  X, y = load_dataset_group('doesntmatter', prefix)\n",
        "  trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "  # zero-offset class values\n",
        "  #trainy = trainy - 1\n",
        "  #testy = testy - 1\n",
        "  # one hot encode y\n",
        "  trainy = to_categorical(trainy)\n",
        "  testy = to_categorical(testy)\n",
        "\n",
        "\n",
        "  return trainX, trainy, testX, testy"
      ],
      "metadata": {
        "id": "objDW_xangKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import LeakyReLU\n",
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "  # define model\n",
        "  verbose, epochs, batch_size = 0, 100, 60\n",
        "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "  # reshape into subsequences (samples, time steps, rows, cols, channels)\n",
        "  n_steps, n_length = 2,22\n",
        "  trainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n",
        "  testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  model.add(ConvLSTM2D(filters=30, kernel_size=(1,1), input_shape=(n_steps, 1, n_length, n_features)))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(LeakyReLU(alpha=0.05))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(100))\n",
        "  model.add(Dense(n_outputs, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # fit network\n",
        "  model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "  # evaluate model\n",
        "  predictions = model.predict(testX)\n",
        "  print(classification_report(testy.argmax(axis=1),predictions.argmax(axis=1)))\n",
        "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "7ODU9IXNmU-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        "\tprint(scores)\n",
        "\tm, s = mean(scores), std(scores)\n",
        "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
      ],
      "metadata": {
        "id": "xPNJKBAxnlDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run an experiment\n",
        "def run_experiment(repeats=3):\n",
        "  # load data\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  scores = list()\n",
        "  for r in range(repeats):\n",
        "    print(\"\\n\")\n",
        "    print(\"Running ConvLSTM with Angular velocity and Acceleration-2D - 9 gestures\")\n",
        "    score = evaluate_model(trainX, trainy, testX, testy)\n",
        "    score = score * 100.0\n",
        "    print('>#%d: %.3f' % (r+1, score))\n",
        "    scores.append(score)\n",
        "  # summarize results\n",
        "  summarize_results(scores)"
      ],
      "metadata": {
        "id": "A5pvnYtUnqZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_NO0Ae0mcPu",
        "outputId": "2b87338f-f40c-4932-95c8-bce3e1318686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Total samples, Timesteps, features) -  (543, 44, 12)\n",
            "\n",
            "\n",
            "Running ConvLSTM with Angular velocity and Acceleration-2D - 9 gestures\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.29      0.40        14\n",
            "           1       0.74      0.70      0.72        20\n",
            "           2       0.47      0.69      0.56        13\n",
            "           3       0.62      0.67      0.65        15\n",
            "           4       0.86      0.92      0.89        13\n",
            "           5       0.40      0.62      0.48        13\n",
            "           6       0.85      0.73      0.79        15\n",
            "           7       0.76      0.93      0.84        14\n",
            "           8       0.83      0.53      0.65        19\n",
            "\n",
            "    accuracy                           0.67       136\n",
            "   macro avg       0.69      0.67      0.66       136\n",
            "weighted avg       0.70      0.67      0.67       136\n",
            "\n",
            ">#1: 66.912\n",
            "\n",
            "\n",
            "Running ConvLSTM with Angular velocity and Acceleration-2D - 9 gestures\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.64      0.58        14\n",
            "           1       0.64      0.80      0.71        20\n",
            "           2       0.54      0.54      0.54        13\n",
            "           3       0.80      0.53      0.64        15\n",
            "           4       0.86      0.92      0.89        13\n",
            "           5       0.47      0.54      0.50        13\n",
            "           6       0.80      0.80      0.80        15\n",
            "           7       0.82      1.00      0.90        14\n",
            "           8       0.90      0.47      0.62        19\n",
            "\n",
            "    accuracy                           0.69       136\n",
            "   macro avg       0.71      0.69      0.69       136\n",
            "weighted avg       0.71      0.69      0.69       136\n",
            "\n",
            ">#2: 69.118\n",
            "\n",
            "\n",
            "Running ConvLSTM with Angular velocity and Acceleration-2D - 9 gestures\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.57      0.62        14\n",
            "           1       0.75      0.75      0.75        20\n",
            "           2       0.43      0.69      0.53        13\n",
            "           3       0.73      0.73      0.73        15\n",
            "           4       0.92      0.92      0.92        13\n",
            "           5       0.57      0.62      0.59        13\n",
            "           6       0.80      0.80      0.80        15\n",
            "           7       0.81      0.93      0.87        14\n",
            "           8       0.90      0.47      0.62        19\n",
            "\n",
            "    accuracy                           0.71       136\n",
            "   macro avg       0.73      0.72      0.71       136\n",
            "weighted avg       0.74      0.71      0.71       136\n",
            "\n",
            ">#3: 71.324\n",
            "[66.91176295280457, 69.11764740943909, 71.32353186607361]\n",
            "Accuracy: 69.118% (+/-1.801)\n"
          ]
        }
      ]
    }
  ]
}
