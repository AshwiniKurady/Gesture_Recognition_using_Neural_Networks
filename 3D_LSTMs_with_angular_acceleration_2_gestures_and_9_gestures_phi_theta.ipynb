{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "T5M0f6xk58CY",
        "ZW7nVnjE6Q9w",
        "3qv21K7keAMe",
        "1N1N9z21B1HY"
      ],
      "authorship_tag": "ABX9TyMr2B6/0+IWHrvOo6YQFdYR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "T5M0f6xk58CY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "import shutil\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "import csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Masking\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from matplotlib import pyplot\n",
        "import sys\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n"
      ],
      "metadata": {
        "id": "ZxU_wSKp5-LG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjXaLLDjq01i",
        "outputId": "95b08956-9a9d-4234-a2cf-b94960768b25"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load a single file as a numpy array\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def load_file(filepath):\n",
        "  tmpList= []\n",
        "  tmpList1 = []\n",
        "  tmpList2 = []\n",
        "  with open(filepath,'r') as thecsv:\n",
        "    for line in thecsv:\n",
        "      line = re.sub(re.compile(r'\\s+'), '', line)\n",
        "      line = line.strip(\",\")\n",
        "      tmpList.append(line.split(',')[0:24])\n",
        "\n",
        "    for i in range(len(tmpList)):\n",
        "      tmpList1 = []\n",
        "      for element in tmpList[i]:\n",
        "        tmpList1.append((float(element)))\n",
        "      tmpList2.append(tmpList1)\n",
        "  dataframe2 = pd.DataFrame(tmpList2)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3.csv')\n",
        "  dataframe2 = dataframe2.replace(np.nan,0.0, regex=True)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3with0.csv')\n",
        "  dataframe = dataframe2.astype(float)\n",
        "  return dataframe.values\n",
        "\n",
        "def load_file_y(filepath):\n",
        "  dataframe_y = read_csv(filepath, header=None)\n",
        "  file = open(filepath, \"r\")\n",
        "  csv_reader = csv. reader(file)\n",
        "  lists_from_csv = []\n",
        "  for row in csv_reader:\n",
        "    lists_from_csv. append(int(row[0])-1)\n",
        "  # return dataframe_y.values.tolist()\n",
        "  return lists_from_csv"
      ],
      "metadata": {
        "id": "BG3-xTLBq3pi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "  loaded = list()\n",
        "  for name in filenames:\n",
        "    data = load_file(prefix + name)\n",
        "    loaded.append(data)\n",
        "\n",
        "  # stack group so that features are the 3rd dimension\n",
        "  loaded = dstack(loaded)\n",
        "  print(\"(Total samples, Timesteps, features) - \", loaded.shape)\n",
        "\n",
        "  return loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "  filepath = prefix + \"/\"\n",
        "  filenames = list()\n",
        "  filenames += ['leftshoulder_velocity.txt', 'leftshoulder_velocity_phi.txt','leftshoulder_acceleration.txt','leftelbow_velocity.txt','leftelbow_velocity_phi.txt','leftelbow_acceleration.txt','leftwrist_velocity.txt','leftwrist_velocity_phi.txt','leftwrist_acceleration.txt',]\n",
        "  filenames += ['rightshoulder_velocity.txt','rightshoulder_velocity_phi.txt','rightshoulder_acceleration.txt','rightelbow_velocity.txt','rightelbow_velocity_phi.txt','rightelbow_acceleration.txt','rightwrist_velocity.txt','rightwrist_velocity_phi.txt','rightwrist_acceleration.txt']\n",
        "  # filenames += ['leftshoulder_polarangle.txt','leftshoulder_velocity.txt','leftelbow_polarangle.txt','leftelbow_velocity.txt','leftwrist_polarangle.txt', 'leftwrist_velocity.txt']\n",
        "  # filenames += ['rightshoulder_polarangle.txt','rightshoulder_velocity.txt','rightelbow_polarangle.txt','rightelbow_velocity.txt','rightwrist_polarangle.txt', 'rightwrist_velocity.txt']\n",
        "  # load input data\n",
        "  X = load_group(filenames, filepath)\n",
        "  # load class output\n",
        "  y = load_file_y( './drive/My Drive/CS298/3D' + '/y.txt')\n",
        "  # print(\"Printing X -----------------------\")\n",
        "  # print(X)\n",
        "  # print(\"Printing y -----------------------\")\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "8TLEg2Kzq6OI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix='./drive/My Drive/CS298/3D/angular_acceleration_6joints_2gestures_phi_theta'):\n",
        "  # load all train\n",
        "  # trainX, trainy = load_dataset_group('train', prefix)\n",
        "  # load all test\n",
        "  X, y = load_dataset_group('doesntmatter', prefix)\n",
        "  trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "  # zero-offset class values\n",
        "  #trainy = trainy - 1\n",
        "  #testy = testy - 1\n",
        "  # one hot encode y\n",
        "  trainy = to_categorical(trainy)\n",
        "  testy = to_categorical(testy)\n",
        "\n",
        "\n",
        "  return trainX, trainy, testX, testy"
      ],
      "metadata": {
        "id": "mZ8xh5eiq8wF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "  verbose, epochs, batch_size = 0, 170, 70\n",
        "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "  print(\"Timesteps - \",n_timesteps, \"Features - \",n_features, \"Output Dimension - \",n_outputs)\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(tf.keras.layers.Masking(mask_value=0.0,input_shape=(n_timesteps, n_features)))\n",
        "  model.add(tf.keras.layers.LSTM(117)) #######need to add the code to directly pick sample -> now it is manually written as 117\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(n_outputs, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # fit network\n",
        "  model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "  # Confusion matrix\n",
        "  predictions = model.predict(testX)\n",
        "  # matrix = confusion_matrix(testy.argmax(axis=1), predictions.argmax(axis=1))\n",
        "  # print(matrix)\n",
        "  print(classification_report(testy.argmax(axis=1), predictions.argmax(axis=1)))\n",
        "  # evaluate model\n",
        "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=2)\n",
        "  # _, accuracy = model.evaluate(testX, testy, verbose=0)\n",
        "  # accuracy = 0\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "qhmhfdeLrBQT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        "  print(scores)\n",
        "  m, s = mean(scores), std(scores)\n",
        "  print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
      ],
      "metadata": {
        "id": "1QcGPgJhrDf1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run an experiment\n",
        "def run_experiment(repeats=3):\n",
        "  # load data\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  scores = list()\n",
        "  for r in range(repeats):\n",
        "    print(\"\\n\")\n",
        "    print(\"Running LSTM with Angular velocity and Acceleration-3D - 2 gestures\")\n",
        "    score = evaluate_model(trainX, trainy, testX, testy)\n",
        "    score = score * 100.0\n",
        "    print('>#%d: %.3f' % (r+1, score))\n",
        "    scores.append(score)\n",
        "  # summarize results\n",
        "  summarize_results(scores)"
      ],
      "metadata": {
        "id": "2V1Mqa4CrF9X"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgQG1dsCrH4N",
        "outputId": "114d504b-99f9-4ba3-c077-420759e6508f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Total samples, Timesteps, features) -  (117, 24, 18)\n",
            "\n",
            "\n",
            "Running LSTM with Angular velocity and Acceleration-3D - 2 gestures\n",
            "Timesteps -  24 Features -  18 Output Dimension -  2\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.41      0.52        17\n",
            "           1       0.50      0.77      0.61        13\n",
            "\n",
            "    accuracy                           0.57        30\n",
            "   macro avg       0.60      0.59      0.56        30\n",
            "weighted avg       0.61      0.57      0.56        30\n",
            "\n",
            "1/1 - 1s - loss: 0.9736 - accuracy: 0.5667 - 1s/epoch - 1s/step\n",
            ">#1: 56.667\n",
            "\n",
            "\n",
            "Running LSTM with Angular velocity and Acceleration-3D - 2 gestures\n",
            "Timesteps -  24 Features -  18 Output Dimension -  2\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.41      0.52        17\n",
            "           1       0.50      0.77      0.61        13\n",
            "\n",
            "    accuracy                           0.57        30\n",
            "   macro avg       0.60      0.59      0.56        30\n",
            "weighted avg       0.61      0.57      0.56        30\n",
            "\n",
            "1/1 - 2s - loss: 0.8273 - accuracy: 0.5667 - 2s/epoch - 2s/step\n",
            ">#2: 56.667\n",
            "\n",
            "\n",
            "Running LSTM with Angular velocity and Acceleration-3D - 2 gestures\n",
            "Timesteps -  24 Features -  18 Output Dimension -  2\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      0.41      0.44        17\n",
            "           1       0.33      0.38      0.36        13\n",
            "\n",
            "    accuracy                           0.40        30\n",
            "   macro avg       0.40      0.40      0.40        30\n",
            "weighted avg       0.41      0.40      0.40        30\n",
            "\n",
            "1/1 - 1s - loss: 1.1334 - accuracy: 0.4000 - 1s/epoch - 1s/step\n",
            ">#3: 40.000\n",
            "[56.66666626930237, 56.66666626930237, 40.00000059604645]\n",
            "Accuracy: 51.111% (+/-7.857)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN LSTM"
      ],
      "metadata": {
        "id": "ZW7nVnjE6Q9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from matplotlib import pyplot\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "q37y1tCk61YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a single file as a numpy array\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def load_file(filepath):\n",
        "  tmpList= []\n",
        "  tmpList1 = []\n",
        "  tmpList2 = []\n",
        "  with open(filepath,'r') as thecsv:\n",
        "    for line in thecsv:\n",
        "      line = re.sub(re.compile(r'\\s+'), '', line)\n",
        "      line = line.strip(\",\")\n",
        "      tmpList.append(line.split(',')[0:24])\n",
        "\n",
        "    for i in range(len(tmpList)):\n",
        "      tmpList1 = []\n",
        "      for element in tmpList[i]:\n",
        "        tmpList1.append((float(element)))\n",
        "      tmpList2.append(tmpList1)\n",
        "  dataframe2 = pd.DataFrame(tmpList2)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3.csv')\n",
        "  dataframe2 = dataframe2.replace(np.nan,0.0, regex=True)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3with0.csv')\n",
        "  dataframe = dataframe2.astype(float)\n",
        "  return dataframe.values\n",
        "\n",
        "def load_file_y(filepath):\n",
        "  dataframe_y = read_csv(filepath, header=None)\n",
        "  file = open(filepath, \"r\")\n",
        "  csv_reader = csv. reader(file)\n",
        "  lists_from_csv = []\n",
        "  for row in csv_reader:\n",
        "    lists_from_csv. append(int(row[0])-1)\n",
        "  # return dataframe_y.values.tolist()\n",
        "  return lists_from_csv"
      ],
      "metadata": {
        "id": "xA0VRZWO6318"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "  loaded = list()\n",
        "  for name in filenames:\n",
        "    data = load_file(prefix + name)\n",
        "    loaded.append(data)\n",
        "\n",
        "  # stack group so that features are the 3rd dimension\n",
        "  loaded = dstack(loaded)\n",
        "  print(\"(Total samples, Timesteps, features) - \", loaded.shape)\n",
        "\n",
        "  return loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "  filepath = prefix + \"/\"\n",
        "  filenames = list()\n",
        "  filenames += ['leftshoulder_velocity.txt', 'leftshoulder_velocity_phi.txt','leftshoulder_acceleration.txt','leftelbow_velocity.txt','leftelbow_velocity_phi.txt','leftelbow_acceleration.txt','leftwrist_velocity.txt','leftwrist_velocity_phi.txt','leftwrist_acceleration.txt',]\n",
        "  filenames += ['rightshoulder_velocity.txt','rightshoulder_velocity_phi.txt','rightshoulder_acceleration.txt','rightelbow_velocity.txt','rightelbow_velocity_phi.txt','rightelbow_acceleration.txt','rightwrist_velocity.txt','rightwrist_velocity_phi.txt','rightwrist_acceleration.txt']\n",
        "  # filenames += ['leftshoulder_polarangle.txt','leftshoulder_velocity.txt','leftelbow_polarangle.txt','leftelbow_velocity.txt','leftwrist_polarangle.txt', 'leftwrist_velocity.txt']\n",
        "  # filenames += ['rightshoulder_polarangle.txt','rightshoulder_velocity.txt','rightelbow_polarangle.txt','rightelbow_velocity.txt','rightwrist_polarangle.txt', 'rightwrist_velocity.txt']\n",
        "  # load input data\n",
        "  X = load_group(filenames, filepath)\n",
        "  # load class output\n",
        "  y = load_file_y( './drive/My Drive/CS298/3D' + '/y.txt')\n",
        "  # print(\"Printing X -----------------------\")\n",
        "  # print(X)\n",
        "  # print(\"Printing y -----------------------\")\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "tTEEp6-f7Kmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix='./drive/My Drive/CS298/3D/angular_acceleration_6joints_2gestures_phi_theta'):\n",
        "  # load all train\n",
        "  # trainX, trainy = load_dataset_group('train', prefix)\n",
        "  # load all test\n",
        "  X, y = load_dataset_group('doesntmatter', prefix)\n",
        "  trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "  # zero-offset class values\n",
        "  #trainy = trainy - 1\n",
        "  #testy = testy - 1\n",
        "  # one hot encode y\n",
        "  trainy = to_categorical(trainy)\n",
        "  testy = to_categorical(testy)\n",
        "\n",
        "\n",
        "  return trainX, trainy, testX, testy"
      ],
      "metadata": {
        "id": "6Yw3yrlu7O10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "  verbose, epochs, batch_size = 0, 60, 60\n",
        "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "  print(\"Timesteps - \",n_timesteps, \"Features - \",n_features, \"Output Dimension - \",n_outputs)\n",
        "  n_steps, n_length = 1,24\n",
        "  trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
        "  testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
        "\t# define model\n",
        "  model = Sequential()\n",
        "  model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n",
        "  model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
        "  model.add(TimeDistributed(Dropout(0.5)))\n",
        "  model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        "  model.add(TimeDistributed(Flatten()))\n",
        "  model.add(LSTM(100))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(n_outputs, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# fit network\n",
        "  model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "\t# evaluate model\n",
        "  predictions = model.predict(testX)\n",
        "  # matrix = confusion_matrix(testy.argmax(axis=1), predictions.argmax(axis=1))\n",
        "  # print(matrix)\n",
        "  print(classification_report(testy.argmax(axis=1), predictions.argmax(axis=1)))\n",
        "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "  return accuracy\n",
        " "
      ],
      "metadata": {
        "id": "koabF3Iw6UDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        "\tprint(scores)\n",
        "\tm, s = mean(scores), std(scores)\n",
        "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
        " "
      ],
      "metadata": {
        "id": "4RB83PeD7r-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run an experiment\n",
        "def run_experiment(repeats=3):\n",
        "  # load data\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  scores = list()\n",
        "  for r in range(repeats):\n",
        "    print(\"\\n\")\n",
        "    print(\"Running CNN-LSTM with Angular velocity and Acceleration-3D - 2 gestures\")\n",
        "    score = evaluate_model(trainX, trainy, testX, testy)\n",
        "    score = score * 100.0\n",
        "    print('>#%d: %.3f' % (r+1, score))\n",
        "    scores.append(score)\n",
        "  # summarize results\n",
        "  summarize_results(scores)"
      ],
      "metadata": {
        "id": "tVVlW8NS8Jw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UTczNw08K9M",
        "outputId": "e0d6bfa6-7559-4ca1-c7c1-c838b2f5b7de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Total samples, Timesteps, features) -  (117, 24, 18)\n",
            "\n",
            "\n",
            "Running CNN-LSTM with Angular velocity and Acceleration-3D - 2 gestures\n",
            "Timesteps -  24 Features -  18 Output Dimension -  2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.82      0.78        17\n",
            "           1       0.73      0.62      0.67        13\n",
            "\n",
            "    accuracy                           0.73        30\n",
            "   macro avg       0.73      0.72      0.72        30\n",
            "weighted avg       0.73      0.73      0.73        30\n",
            "\n",
            ">#1: 73.333\n",
            "\n",
            "\n",
            "Running CNN-LSTM with Angular velocity and Acceleration-3D - 2 gestures\n",
            "Timesteps -  24 Features -  18 Output Dimension -  2\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f341feec710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.71      0.71        17\n",
            "           1       0.62      0.62      0.62        13\n",
            "\n",
            "    accuracy                           0.67        30\n",
            "   macro avg       0.66      0.66      0.66        30\n",
            "weighted avg       0.67      0.67      0.67        30\n",
            "\n",
            ">#2: 66.667\n",
            "\n",
            "\n",
            "Running CNN-LSTM with Angular velocity and Acceleration-3D - 2 gestures\n",
            "Timesteps -  24 Features -  18 Output Dimension -  2\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3426ab5050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.82      0.78        17\n",
            "           1       0.73      0.62      0.67        13\n",
            "\n",
            "    accuracy                           0.73        30\n",
            "   macro avg       0.73      0.72      0.72        30\n",
            "weighted avg       0.73      0.73      0.73        30\n",
            "\n",
            ">#3: 73.333\n",
            "[73.33333492279053, 66.66666865348816, 73.33333492279053]\n",
            "Accuracy: 71.111% (+/-3.143)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conv LSTM - 2 gestures"
      ],
      "metadata": {
        "id": "3qv21K7keAMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import ConvLSTM2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from matplotlib import pyplot"
      ],
      "metadata": {
        "id": "mTpOvPkneC0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a single file as a numpy array\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def load_file(filepath):\n",
        "  tmpList= []\n",
        "  tmpList1 = []\n",
        "  tmpList2 = []\n",
        "  with open(filepath,'r') as thecsv:\n",
        "    for line in thecsv:\n",
        "      line = re.sub(re.compile(r'\\s+'), '', line)\n",
        "      line = line.strip(\",\")\n",
        "      tmpList.append(line.split(',')[0:24])\n",
        "\n",
        "    for i in range(len(tmpList)):\n",
        "      tmpList1 = []\n",
        "      for element in tmpList[i]:\n",
        "        tmpList1.append((float(element)))\n",
        "      tmpList2.append(tmpList1)\n",
        "  dataframe2 = pd.DataFrame(tmpList2)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3.csv')\n",
        "  dataframe2 = dataframe2.replace(np.nan,0.0, regex=True)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3with0.csv')\n",
        "  dataframe = dataframe2.astype(float)\n",
        "  return dataframe.values\n",
        "\n",
        "def load_file_y(filepath):\n",
        "  dataframe_y = read_csv(filepath, header=None)\n",
        "  file = open(filepath, \"r\")\n",
        "  csv_reader = csv. reader(file)\n",
        "  lists_from_csv = []\n",
        "  for row in csv_reader:\n",
        "    lists_from_csv. append(int(row[0])-1)\n",
        "  # return dataframe_y.values.tolist()\n",
        "  return lists_from_csv"
      ],
      "metadata": {
        "id": "JPAIcietedt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "  loaded = list()\n",
        "  for name in filenames:\n",
        "    data = load_file(prefix + name)\n",
        "    loaded.append(data)\n",
        "\n",
        "  # stack group so that features are the 3rd dimension\n",
        "  loaded = dstack(loaded)\n",
        "  print(\"(Total samples, Timesteps, features) - \", loaded.shape)\n",
        "\n",
        "  return loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "  filepath = prefix + \"/\"\n",
        "  filenames = list()\n",
        "  filenames += ['leftshoulder_velocity.txt', 'leftshoulder_velocity_phi.txt','leftshoulder_acceleration.txt','leftelbow_velocity.txt','leftelbow_velocity_phi.txt','leftelbow_acceleration.txt','leftwrist_velocity.txt','leftwrist_velocity_phi.txt','leftwrist_acceleration.txt',]\n",
        "  filenames += ['rightshoulder_velocity.txt','rightshoulder_velocity_phi.txt','rightshoulder_acceleration.txt','rightelbow_velocity.txt','rightelbow_velocity_phi.txt','rightelbow_acceleration.txt','rightwrist_velocity.txt','rightwrist_velocity_phi.txt','rightwrist_acceleration.txt']\n",
        "  # filenames += ['leftshoulder_polarangle.txt','leftshoulder_velocity.txt','leftelbow_polarangle.txt','leftelbow_velocity.txt','leftwrist_polarangle.txt', 'leftwrist_velocity.txt']\n",
        "  # filenames += ['rightshoulder_polarangle.txt','rightshoulder_velocity.txt','rightelbow_polarangle.txt','rightelbow_velocity.txt','rightwrist_polarangle.txt', 'rightwrist_velocity.txt']\n",
        "  # load input data\n",
        "  X = load_group(filenames, filepath)\n",
        "  # load class output\n",
        "  y = load_file_y( './drive/My Drive/CS298/3D' + '/y.txt')\n",
        "  # print(\"Printing X -----------------------\")\n",
        "  # print(X)\n",
        "  # print(\"Printing y -----------------------\")\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "PlKu998wexiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix='./drive/MyDrive/CS298/3D/angular_acceleration_6joints_2gestures_phi_theta'):\n",
        "  # load all train\n",
        "  # trainX, trainy = load_dataset_group('train', prefix)\n",
        "  # load all test\n",
        "  X, y = load_dataset_group('doesntmatter', prefix)\n",
        "  trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "  # zero-offset class values\n",
        "  #trainy = trainy - 1\n",
        "  #testy = testy - 1\n",
        "  # one hot encode y\n",
        "  trainy = to_categorical(trainy)\n",
        "  testy = to_categorical(testy)\n",
        "\n",
        "\n",
        "  return trainX, trainy, testX, testy"
      ],
      "metadata": {
        "id": "JMbXSWBCe2c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "\t# define model\n",
        "\tverbose, epochs, batch_size = 0, 60 , 117\n",
        "\tn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "\t# reshape into subsequences (samples, time steps, rows, cols, channels)\n",
        "\tn_steps, n_length = 1,24\n",
        "\ttrainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n",
        "\ttestX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(ConvLSTM2D(filters=60, kernel_size=(1,1), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(100, activation='relu'))\n",
        "\tmodel.add(Dense(n_outputs, activation='softmax'))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# fit network\n",
        "\tmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "\t# evaluate model\n",
        "\tpredictions = model.predict(testX)\n",
        "\tprint(classification_report(testy.argmax(axis=1), predictions.argmax(axis=1)))\n",
        "\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "\treturn accuracy"
      ],
      "metadata": {
        "id": "KOGsNuC2e_RO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        "\tprint(scores)\n",
        "\tm, s = mean(scores), std(scores)\n",
        "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
      ],
      "metadata": {
        "id": "bo7IAs8df2GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run an experiment\n",
        "def run_experiment(repeats=3):\n",
        "  # load data\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  scores = list()\n",
        "  for r in range(repeats):\n",
        "    print(\"\\n\")\n",
        "    print(\"Running ConvLSTM with Angular velocity and Acceleration-3D - 2 gestures\")\n",
        "    score = evaluate_model(trainX, trainy, testX, testy)\n",
        "    score = score * 100.0\n",
        "    print('>#%d: %.3f' % (r+1, score))\n",
        "    scores.append(score)\n",
        "  # summarize results\n",
        "  summarize_results(scores)"
      ],
      "metadata": {
        "id": "VGCHkwi7f5uG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_experiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtuWUvukf8w6",
        "outputId": "7d7900f2-a1d2-4104-ec38-7ff10a94a2f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Total samples, Timesteps, features) -  (117, 24, 18)\n",
            "\n",
            "\n",
            "Running ConvLSTM with Angular velocity and Acceleration-3D - 2 gestures\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.76      0.74        17\n",
            "           1       0.67      0.62      0.64        13\n",
            "\n",
            "    accuracy                           0.70        30\n",
            "   macro avg       0.69      0.69      0.69        30\n",
            "weighted avg       0.70      0.70      0.70        30\n",
            "\n",
            ">#1: 70.000\n",
            "\n",
            "\n",
            "Running ConvLSTM with Angular velocity and Acceleration-3D - 2 gestures\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.94      0.84        17\n",
            "           1       0.89      0.62      0.73        13\n",
            "\n",
            "    accuracy                           0.80        30\n",
            "   macro avg       0.83      0.78      0.78        30\n",
            "weighted avg       0.82      0.80      0.79        30\n",
            "\n",
            ">#2: 80.000\n",
            "\n",
            "\n",
            "Running ConvLSTM with Angular velocity and Acceleration-3D - 2 gestures\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.94      0.82        17\n",
            "           1       0.88      0.54      0.67        13\n",
            "\n",
            "    accuracy                           0.77        30\n",
            "   macro avg       0.80      0.74      0.74        30\n",
            "weighted avg       0.79      0.77      0.75        30\n",
            "\n",
            ">#3: 76.667\n",
            "[69.9999988079071, 80.0000011920929, 76.66666507720947]\n",
            "Accuracy: 75.556% (+/-4.157)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ConvLSTM - 9 gestures"
      ],
      "metadata": {
        "id": "1N1N9z21B1HY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import ConvLSTM2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from matplotlib import pyplot"
      ],
      "metadata": {
        "id": "udBs6FdgB-1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a single file as a numpy array\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def load_file(filepath):\n",
        "  tmpList= []\n",
        "  tmpList1 = []\n",
        "  tmpList2 = []\n",
        "  with open(filepath,'r') as thecsv:\n",
        "    for line in thecsv:\n",
        "      line = re.sub(re.compile(r'\\s+'), '', line)\n",
        "      line = line.strip(\",\")\n",
        "      tmpList.append(line.split(',')[0:30])\n",
        "\n",
        "    for i in range(len(tmpList)):\n",
        "      tmpList1 = []\n",
        "      for element in tmpList[i]:\n",
        "        tmpList1.append((float(element)))\n",
        "      tmpList2.append(tmpList1)\n",
        "  dataframe2 = pd.DataFrame(tmpList2)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3.csv')\n",
        "  dataframe2 = dataframe2.replace(np.nan,0.0, regex=True)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3with0.csv')\n",
        "  dataframe = dataframe2.astype(float)\n",
        "  return dataframe.values\n",
        "\n",
        "def load_file_y(filepath):\n",
        "  dataframe_y = read_csv(filepath, header=None)\n",
        "  file = open(filepath, \"r\")\n",
        "  csv_reader = csv. reader(file)\n",
        "  lists_from_csv = []\n",
        "  for row in csv_reader:\n",
        "    lists_from_csv. append(int(row[0])-1)\n",
        "  # return dataframe_y.values.tolist()\n",
        "  return lists_from_csv"
      ],
      "metadata": {
        "id": "MxiS-4W4CGIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "  loaded = list()\n",
        "  for name in filenames:\n",
        "    data = load_file(prefix + name)\n",
        "    loaded.append(data)\n",
        "\n",
        "  # stack group so that features are the 3rd dimension\n",
        "  loaded = dstack(loaded)\n",
        "  print(\"(Total samples, Timesteps, features) - \", loaded.shape)\n",
        "\n",
        "  return loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "  filepath = prefix + \"/\"\n",
        "  filenames = list()\n",
        "  filenames += ['leftshoulder_velocity.txt', 'leftshoulder_velocity_phi.txt','leftshoulder_acceleration.txt','leftelbow_velocity.txt','leftelbow_velocity_phi.txt','leftelbow_acceleration.txt','leftwrist_velocity.txt','leftwrist_velocity_phi.txt','leftwrist_acceleration.txt',]\n",
        "  filenames += ['rightshoulder_velocity.txt','rightshoulder_velocity_phi.txt','rightshoulder_acceleration.txt','rightelbow_velocity.txt','rightelbow_velocity_phi.txt','rightelbow_acceleration.txt','rightwrist_velocity.txt','rightwrist_velocity_phi.txt','rightwrist_acceleration.txt']\n",
        "  # filenames += ['leftshoulder_polarangle.txt','leftshoulder_velocity.txt','leftelbow_polarangle.txt','leftelbow_velocity.txt','leftwrist_polarangle.txt', 'leftwrist_velocity.txt']\n",
        "  # filenames += ['rightshoulder_polarangle.txt','rightshoulder_velocity.txt','rightelbow_polarangle.txt','rightelbow_velocity.txt','rightwrist_polarangle.txt', 'rightwrist_velocity.txt']\n",
        "  # load input data\n",
        "  X = load_group(filenames, filepath)\n",
        "  # load class output\n",
        "  y = load_file_y( './drive/My Drive/CS298/3D' + '/y_9gestures.txt')\n",
        "  # print(\"Printing X -----------------------\")\n",
        "  # print(X)\n",
        "  # print(\"Printing y -----------------------\")\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "_NlPa3WyCMVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix='./drive/MyDrive/CS298/3D/angular_acceleration_6joints_9gestures_phi_theta'):\n",
        "  # load all train\n",
        "  # trainX, trainy = load_dataset_group('train', prefix)\n",
        "  # load all test\n",
        "  X, y = load_dataset_group('doesntmatter', prefix)\n",
        "  trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "  # zero-offset class values\n",
        "  #trainy = trainy - 1\n",
        "  #testy = testy - 1\n",
        "  # one hot encode y\n",
        "  trainy = to_categorical(trainy)\n",
        "  testy = to_categorical(testy)\n",
        "\n",
        "\n",
        "  return trainX, trainy, testX, testy"
      ],
      "metadata": {
        "id": "YegR8o-nCTWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import LeakyReLU\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "  # define model\n",
        "  verbose, epochs, batch_size = 0, 50 , 60\n",
        "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "  # reshape into subsequences (samples, time steps, rows, cols, channels)\n",
        "  n_steps, n_length = 2,15\n",
        "  trainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n",
        "  testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  model.add(ConvLSTM2D(filters=121, kernel_size=(1,2), activation='tanh', input_shape=(n_steps, 1, n_length, n_features)))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(LeakyReLU(alpha=0.05))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1000, activation='tanh'))\n",
        "  model.add(Dense(n_outputs, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # fit network\n",
        "  model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "  # evaluate model\n",
        "  predictions = model.predict(testX)\n",
        "  print(classification_report(testy.argmax(axis=1), predictions.argmax(axis=1)))\n",
        "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "AqodfiAaCUJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        "\tprint(scores)\n",
        "\tm, s = mean(scores), std(scores)\n",
        "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
      ],
      "metadata": {
        "id": "ONRo0HQyCcvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run an experiment\n",
        "def run_experiment(repeats=3):\n",
        "  # load data\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  scores = list()\n",
        "  for r in range(repeats):\n",
        "    print(\"\\n\")\n",
        "    print(\"Running ConvLSTM with Angular velocity and Acceleration-3D - 9 gestures\")\n",
        "    score = evaluate_model(trainX, trainy, testX, testy)\n",
        "    \n",
        "    score = score * 100.0\n",
        "    print('>#%d: %.3f' % (r+1, score))\n",
        "    scores.append(score)\n",
        "  # summarize results\n",
        "  summarize_results(scores)"
      ],
      "metadata": {
        "id": "WVrtOPP9Cg5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_experiment()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xqNd-9DCh6h",
        "outputId": "92e4b59c-cd70-4db6-b81b-4f9640e9d395"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Total samples, Timesteps, features) -  (543, 30, 18)\n",
            "\n",
            "\n",
            "Running ConvLSTM with Angular velocity and Acceleration-3D - 9 gestures\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.50      0.44        14\n",
            "           1       0.77      0.50      0.61        20\n",
            "           2       0.17      0.23      0.19        13\n",
            "           3       0.53      0.67      0.59        15\n",
            "           4       0.73      0.85      0.79        13\n",
            "           5       0.18      0.38      0.24        13\n",
            "           6       0.40      0.13      0.20        15\n",
            "           7       0.67      0.57      0.62        14\n",
            "           8       0.50      0.21      0.30        19\n",
            "\n",
            "    accuracy                           0.44       136\n",
            "   macro avg       0.48      0.45      0.44       136\n",
            "weighted avg       0.50      0.44      0.44       136\n",
            "\n",
            ">#1: 44.118\n",
            "\n",
            "\n",
            "Running ConvLSTM with Angular velocity and Acceleration-3D - 9 gestures\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.43      0.39        14\n",
            "           1       0.69      0.55      0.61        20\n",
            "           2       0.36      0.31      0.33        13\n",
            "           3       0.37      0.73      0.49        15\n",
            "           4       0.69      0.85      0.76        13\n",
            "           5       0.11      0.15      0.12        13\n",
            "           6       0.40      0.13      0.20        15\n",
            "           7       0.62      0.57      0.59        14\n",
            "           8       0.33      0.16      0.21        19\n",
            "\n",
            "    accuracy                           0.43       136\n",
            "   macro avg       0.43      0.43      0.41       136\n",
            "weighted avg       0.44      0.43      0.41       136\n",
            "\n",
            ">#2: 42.647\n",
            "\n",
            "\n",
            "Running ConvLSTM with Angular velocity and Acceleration-3D - 9 gestures\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.43      0.41        14\n",
            "           1       0.63      0.60      0.62        20\n",
            "           2       0.17      0.23      0.19        13\n",
            "           3       0.28      0.60      0.38        15\n",
            "           4       0.64      0.69      0.67        13\n",
            "           5       0.42      0.38      0.40        13\n",
            "           6       0.83      0.33      0.48        15\n",
            "           7       0.70      0.50      0.58        14\n",
            "           8       0.50      0.26      0.34        19\n",
            "\n",
            "    accuracy                           0.45       136\n",
            "   macro avg       0.51      0.45      0.45       136\n",
            "weighted avg       0.52      0.45      0.46       136\n",
            "\n",
            ">#3: 44.853\n",
            "[44.11764740943909, 42.64705777168274, 44.85294222831726]\n",
            "Accuracy: 43.873% (+/-0.917)\n"
          ]
        }
      ]
    }
  ]
}