{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaW0fql4zBWhbxbyV8m8rk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "Ne-1XqnaPr1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "import shutil\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "import csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Masking\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from matplotlib import pyplot\n",
        "import sys\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ],
      "metadata": {
        "id": "TiEOpfrhPwOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjXaLLDjq01i",
        "outputId": "d01cb8ff-98a4-4494-a4be-c9b314d30f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load a single file as a numpy array\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def load_file(filepath):\n",
        "  tmpList= []\n",
        "  tmpList1 = []\n",
        "  tmpList2 = []\n",
        "  with open(filepath,'r') as thecsv:\n",
        "    for line in thecsv:\n",
        "      line = re.sub(re.compile(r'\\s+'), '', line)\n",
        "      line = line.strip(\",\")\n",
        "      tmpList.append(line.split(',')[0:24])\n",
        "\n",
        "    for i in range(len(tmpList)):\n",
        "      tmpList1 = []\n",
        "      for element in tmpList[i]:\n",
        "        tmpList1.append((float(element)))\n",
        "      tmpList2.append(tmpList1)\n",
        "  dataframe2 = pd.DataFrame(tmpList2)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3.csv')\n",
        "  dataframe2 = dataframe2.replace(np.nan,0.0, regex=True)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3with0.csv')\n",
        "  dataframe = dataframe2.astype(float)\n",
        "  return dataframe.values\n",
        "\n",
        "def load_file_y(filepath):\n",
        "  dataframe_y = read_csv(filepath, header=None)\n",
        "  file = open(filepath, \"r\")\n",
        "  csv_reader = csv. reader(file)\n",
        "  lists_from_csv = []\n",
        "  for row in csv_reader:\n",
        "    lists_from_csv. append(int(row[0])-1)\n",
        "  # return dataframe_y.values.tolist()\n",
        "  return lists_from_csv"
      ],
      "metadata": {
        "id": "BG3-xTLBq3pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "  loaded = list()\n",
        "  for name in filenames:\n",
        "    data = load_file(prefix + name)\n",
        "    loaded.append(data) \n",
        "  # stack group so that features are the 3rd dimension\n",
        "  loaded = dstack(loaded)\n",
        "  print(\"(Total samples, Timesteps, features) - \", loaded.shape)\n",
        "  return loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "  filepath = prefix + \"/\"\n",
        "  filenames = list()\n",
        "  # filenames += ['leftshoulder_velocity.txt','leftshoulder_acceleration.txt','leftelbow_velocity.txt','leftelbow_acceleration.txt','leftwrist_velocity.txt', 'leftwrist_acceleration.txt',]\n",
        "  # filenames += ['rightshoulder_velocity.txt','rightshoulder_acceleration.txt','rightelbow_velocity.txt','rightelbow_acceleration.txt','rightwrist_velocity.txt', 'rightwrist_acceleration.txt',]\n",
        "  filenames += ['leftshoulder_polarangle.txt','leftshoulder_velocity.txt','leftelbow_polarangle.txt','leftelbow_velocity.txt','leftwrist_polarangle.txt', 'leftwrist_velocity.txt']\n",
        "  filenames += ['rightshoulder_polarangle.txt','rightshoulder_velocity.txt','rightelbow_polarangle.txt','rightelbow_velocity.txt','rightwrist_polarangle.txt', 'rightwrist_velocity.txt']\n",
        "  # load input data\n",
        "  X = load_group(filenames, filepath)\n",
        "  # load class output\n",
        "  y = load_file_y( './drive/My Drive/CS298/2D/' + '/y.txt')\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "8TLEg2Kzq6OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix='./drive/My Drive/CS298/2D/polar_angular_6joints'):\n",
        "  # load all train\n",
        "  # trainX, trainy = load_dataset_group('train', prefix)\n",
        "  # load all test\n",
        "  X, y = load_dataset_group('doesntmatter', prefix)\n",
        "  trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "  # zero-offset class values\n",
        "  # trainy = trainy - 1\n",
        "  # testy = testy - 1\n",
        "  # one hot encode y\n",
        "  trainy = to_categorical(trainy)\n",
        "  testy = to_categorical(testy)\n",
        "\n",
        "  return trainX, trainy, testX, testy"
      ],
      "metadata": {
        "id": "mZ8xh5eiq8wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "  verbose, epochs, batch_size = 0, 20, 117\n",
        "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "  print(\"Timesteps - \",n_timesteps, \"Features - \",n_features, \"Output Dimension - \",n_outputs)\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(tf.keras.layers.Masking(mask_value=0.0,input_shape=(n_timesteps, n_features)))\n",
        "  model.add(tf.keras.layers.LSTM(117)) #######need to add the code to directly pick sample -> now it is manually written as 117\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(n_outputs, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # fit network\n",
        "  model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "  # Confusion matrix\n",
        "  predictions = model.predict(testX)\n",
        "  # matrix = confusion_matrix(testy.argmax(axis=1), predictions.argmax(axis=1))\n",
        "  # print(matrix)\n",
        "  print(classification_report(testy.argmax(axis=1), predictions.argmax(axis=1)))\n",
        "  # evaluate model\n",
        "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=2)\n",
        "  # _, accuracy = model.evaluate(testX, testy, verbose=0)\n",
        "  # accuracy = 0\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "qhmhfdeLrBQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        "  print(scores)\n",
        "  m, s = mean(scores), std(scores)\n",
        "  print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
      ],
      "metadata": {
        "id": "1QcGPgJhrDf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run an experiment\n",
        "def run_experiment(repeats=3):\n",
        "  # load data\n",
        "\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  scores = list()\n",
        "  for r in range(repeats):\n",
        "    print(\"\\n\")\n",
        "    print(\"Running LSTM with Polar angle and Angular velocity - 2 gestures\" )\n",
        "    score = evaluate_model(trainX, trainy, testX, testy)\n",
        "    score = score * 100.0\n",
        "    print('>#%d: %.3f' % (r+1, score))\n",
        "    scores.append(score)\n",
        "  # summarize results\n",
        "  summarize_results(scores)"
      ],
      "metadata": {
        "id": "2V1Mqa4CrF9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_experiment()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgQG1dsCrH4N",
        "outputId": "10470ca6-78c6-4bbe-f945-f93eb562a88d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Total samples, Timesteps, features) -  (117, 24, 12)\n",
            "\n",
            "\n",
            "Running LSTM with Polar angle and Angular velocity - 2 gestures\n",
            "Timesteps -  24 Features -  12 Output Dimension -  2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.71      0.73        17\n",
            "           1       0.64      0.69      0.67        13\n",
            "\n",
            "    accuracy                           0.70        30\n",
            "   macro avg       0.70      0.70      0.70        30\n",
            "weighted avg       0.70      0.70      0.70        30\n",
            "\n",
            "1/1 - 2s - loss: 0.6303 - accuracy: 0.7000 - 2s/epoch - 2s/step\n",
            ">#1: 70.000\n",
            "\n",
            "\n",
            "Running LSTM with Polar angle and Angular velocity - 2 gestures\n",
            "Timesteps -  24 Features -  12 Output Dimension -  2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.29      0.43        17\n",
            "           1       0.50      0.92      0.65        13\n",
            "\n",
            "    accuracy                           0.57        30\n",
            "   macro avg       0.67      0.61      0.54        30\n",
            "weighted avg       0.69      0.57      0.53        30\n",
            "\n",
            "1/1 - 2s - loss: 0.7704 - accuracy: 0.5667 - 2s/epoch - 2s/step\n",
            ">#2: 56.667\n",
            "\n",
            "\n",
            "Running LSTM with Polar angle and Angular velocity - 2 gestures\n",
            "Timesteps -  24 Features -  12 Output Dimension -  2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.35      0.50        17\n",
            "           1       0.52      0.92      0.67        13\n",
            "\n",
            "    accuracy                           0.60        30\n",
            "   macro avg       0.69      0.64      0.58        30\n",
            "weighted avg       0.71      0.60      0.57        30\n",
            "\n",
            "1/1 - 1s - loss: 0.6602 - accuracy: 0.6000 - 1s/epoch - 1s/step\n",
            ">#3: 60.000\n",
            "[69.9999988079071, 56.66666626930237, 60.00000238418579]\n",
            "Accuracy: 62.222% (+/-5.666)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN and LSTM"
      ],
      "metadata": {
        "id": "6rdYXSu5Pzjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "import shutil\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "import csv\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Masking\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from matplotlib import pyplot\n",
        "import sys\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "5tLBVd47P19p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kT9EfaXnP8ex",
        "outputId": "11b08a54-af5c-4b42-f2e2-1f83d38813c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load a single file as a numpy array\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def load_file(filepath):\n",
        "  tmpList= []\n",
        "  tmpList1 = []\n",
        "  tmpList2 = []\n",
        "  with open(filepath,'r') as thecsv:\n",
        "    for line in thecsv:\n",
        "      line = re.sub(re.compile(r'\\s+'), '', line)\n",
        "      line = line.strip(\",\")\n",
        "      tmpList.append(line.split(','))\n",
        "\n",
        "    for i in range(len(tmpList)):\n",
        "      tmpList1 = []\n",
        "      for element in tmpList[i]:\n",
        "        tmpList1.append((float(element)))\n",
        "      tmpList2.append(tmpList1)\n",
        "  dataframe2 = pd.DataFrame(tmpList2)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3.csv')\n",
        "  dataframe2 = dataframe2.replace(np.nan,0.0, regex=True)\n",
        "  # dataframe2.to_csv('./drive/My Drive/Ashwini/gestureData/test1/dataframe3with0.csv')\n",
        "  dataframe = dataframe2.astype(float)\n",
        "  return dataframe.values\n",
        "\n",
        "def load_file_y(filepath):\n",
        "  dataframe_y = read_csv(filepath, header=None)\n",
        "  return dataframe_y.values"
      ],
      "metadata": {
        "id": "EUnkoqmBQAAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "  loaded = list()\n",
        "  for name in filenames:\n",
        "    data = load_file(prefix + name)\n",
        "    loaded.append(data) \n",
        "  # stack group so that features are the 3rd dimension\n",
        "  loaded = dstack(loaded)\n",
        "  print(\"(Total samples, Timesteps, features) - \", loaded.shape)\n",
        "  return loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "  filepath = prefix + \"/\"\n",
        "  filenames = list()\n",
        "  # filenames += ['leftshoulder_velocity.txt','leftshoulder_acceleration.txt','leftelbow_velocity.txt','leftelbow_acceleration.txt','leftwrist_velocity.txt', 'leftwrist_acceleration.txt',]\n",
        "  # filenames += ['rightshoulder_velocity.txt','rightshoulder_acceleration.txt','rightelbow_velocity.txt','rightelbow_acceleration.txt','rightwrist_velocity.txt', 'rightwrist_acceleration.txt',]\n",
        "  filenames += ['leftshoulder_polarangle.txt','leftshoulder_velocity.txt','leftelbow_polarangle.txt','leftelbow_velocity.txt','leftwrist_polarangle.txt', 'leftwrist_velocity.txt']\n",
        "  filenames += ['rightshoulder_polarangle.txt','rightshoulder_velocity.txt','rightelbow_polarangle.txt','rightelbow_velocity.txt','rightwrist_polarangle.txt', 'rightwrist_velocity.txt']\n",
        "  # load input data\n",
        "  X = load_group(filenames, filepath)\n",
        "  # load class output\n",
        "  y = load_file_y( './drive/My Drive/CS298/' + '/y.txt')\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "IFMwxP-2QDyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix='./drive/My Drive/CS298/polar_angular_6joints'):\n",
        "  # load all train\n",
        "  # trainX, trainy = load_dataset_group('train', prefix)\n",
        "  # load all test\n",
        "  X, y = load_dataset_group('doesntmatter', prefix)\n",
        "  trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "  # zero-offset class values\n",
        "  trainy = trainy - 1\n",
        "  testy = testy - 1\n",
        "  # one hot encode y\n",
        "  #print(\"Before_Categorization\",trainy.shape,testy.shape)\n",
        "  trainy = to_categorical(trainy)\n",
        "  testy = to_categorical(testy)\n",
        "  #print(\"After_Categorization\",trainy.shape,testy.shape)\n",
        "  return trainX, trainy, testX, testy"
      ],
      "metadata": {
        "id": "ANNf5NqiQIcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "  verbose, epochs, batch_size = 0, 30, 64\n",
        "  n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "  print(\"Timesteps - \",n_timesteps, \"Features - \",n_features, \"Output Dimension - \",n_outputs)\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(tf.keras.layers.Masking(mask_value=0.0,input_shape=(n_timesteps, n_features)))\n",
        "  model.add(tf.keras.layers.LSTM(117)) #######need to add the code to directly pick sample -> now it is manually written as 117\n",
        "  model.add(Dense(100, activation='tanh'))\n",
        "  model.add(Dense(n_outputs, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  # fit network\n",
        "  model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "  # Confusion matrix\n",
        "  predictions = model.predict(testX)\n",
        "  # matrix = confusion_matrix(testy.argmax(axis=1), predictions.argmax(axis=1))\n",
        "  # print(matrix)\n",
        "  print(classification_report(testy.argmax(axis=1), predictions.argmax(axis=1)))\n",
        "  # evaluate model\n",
        "  _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=2)\n",
        "  # _, accuracy = model.evaluate(testX, testy, verbose=0)\n",
        "  # accuracy = 0\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "n_f6fqB1RN3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "\t# define model\n",
        "\tverbose, epochs, batch_size = 0, 25, 60\n",
        "\tn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "\t# reshape data into time steps of sub-sequences\n",
        "\tn_steps, n_length = 4,10\n",
        "\ttrainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
        "\ttestX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
        "\t# define model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n",
        "\tmodel.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
        "\tmodel.add(TimeDistributed(Dropout(0.5)))\n",
        "\tmodel.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
        "\tmodel.add(TimeDistributed(Flatten()))\n",
        "\tmodel.add(LSTM(100))\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\tmodel.add(Dense(100, activation='relu'))\n",
        "\tmodel.add(Dense(n_outputs, activation='softmax'))\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\t# fit network\n",
        "\tmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "\t# evaluate model\n",
        "\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "\treturn accuracy"
      ],
      "metadata": {
        "id": "rrL8bnm7QMlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        "  print(scores)\n",
        "  m, s = mean(scores), std(scores)\n",
        "  print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
      ],
      "metadata": {
        "id": "n82E4eZgQNvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run an experiment\n",
        "def run_experiment(repeats=3):\n",
        "  # load data\n",
        "\n",
        "  trainX, trainy, testX, testy = load_dataset()\n",
        "  scores = list()\n",
        "  for r in range(repeats):\n",
        "    print(\"\\n\")\n",
        "    print(\"Running LSTM with Polar angle and Angular velocity - 2 gestures\" )\n",
        "    score = evaluate_model(trainX, trainy, testX, testy)\n",
        "    score = score * 100.0\n",
        "    print('>#%d: %.3f' % (r+1, score))\n",
        "    scores.append(score)\n",
        "  # summarize results\n",
        "  summarize_results(scores)"
      ],
      "metadata": {
        "id": "EWv2cs1CQTwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_experiment()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "Noz3f46-QWgM",
        "outputId": "493a8115-d647-49ec-e9c4-d3a45e086b10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Total samples, Timesteps, features) -  (117, 43, 12)\n",
            "\n",
            "\n",
            "Running LSTM with Polar angle and Angular velocity - 2 gestures\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-400188e9bc26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-aebeefd7f591>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(repeats)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running LSTM with Polar angle and Angular velocity - 2 gestures\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>#%d: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-871d99456f38>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(trainX, trainy, testX, testy)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# reshape data into time steps of sub-sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtrainX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mtestX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# define model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 44892 into shape (87,4,10,12)"
          ]
        }
      ]
    }
  ]
}